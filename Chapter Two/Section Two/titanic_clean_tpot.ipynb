{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbaseconda68d4c1b18059495ab3f7b8c839aeb166",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用平均年龄来填充年龄中nan值\n",
    "train_data['Age'].fillna(train_data['Age'].mean(),inplace=True)\n",
    "test_data['Age'].fillna(test_data['Age'].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用票价的均值填充票价中的nan值\n",
    "train_data['Fare'].fillna(train_data['Fare'].mean(), inplace=True)\n",
    "test_data['Fare'].fillna(test_data['Fare'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "S    644\nC    168\nQ     77\nName: Embarked, dtype: int64\n"
    }
   ],
   "source": [
    "print(train_data['Embarked'].value_counts())\n",
    "# 使用登录最多的港口来填充登录港口的nan值\n",
    "train_data['Embarked'].fillna('S',inplace=True)\n",
    "test_data['Embarked'].fillna('S',inplace=True)\n",
    "test_data['Embarked'].replace('S',1,inplace=True)\n",
    "test_data['Embarked'].replace('C',2,inplace=True)\n",
    "test_data['Embarked'].replace('Q',3,inplace=True)\n",
    "train_data['Embarked'].replace('S',1,inplace=True)\n",
    "train_data['Embarked'].replace('C',2,inplace=True)\n",
    "train_data['Embarked'].replace('Q',3,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "特征值\n     Pclass  Sex        Age  SibSp  Parch     Fare  Embarked\n0         3    0  22.000000      1      0   7.2500         1\n1         1    1  38.000000      1      0  71.2833         2\n2         3    1  26.000000      0      0   7.9250         1\n3         1    1  35.000000      1      0  53.1000         1\n4         3    0  35.000000      0      0   8.0500         1\n..      ...  ...        ...    ...    ...      ...       ...\n886       2    0  27.000000      0      0  13.0000         1\n887       1    1  19.000000      0      0  30.0000         1\n888       3    1  29.699118      1      2  23.4500         1\n889       1    0  26.000000      0      0  30.0000         2\n890       3    0  32.000000      0      0   7.7500         3\n\n[891 rows x 7 columns]\n"
    }
   ],
   "source": [
    "# 特征选择\n",
    "features = ['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']\n",
    "train_data['Sex'].replace('female',1,inplace=True)\n",
    "train_data['Sex'].replace('male',0,inplace=True)\n",
    "test_data['Sex'].replace('female',1,inplace=True)\n",
    "test_data['Sex'].replace('male',0,inplace=True)\n",
    "train_features = train_data[features]\n",
    "train_labels = train_data['Survived']\n",
    "test_features = test_data[features]\n",
    "print('特征值')\n",
    "print(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Optimization Progress:  33%|███▎      | 40/120 [00:13<00:41,  1.92pipeline/s]Generation 1 - Current best internal CV score: 0.824938798568828\nOptimization Progress:  49%|████▉     | 59/120 [00:20<00:42,  1.44pipeline/s]Generation 2 - Current best internal CV score: 0.835044881049526\nOptimization Progress:  67%|██████▋   | 80/120 [00:32<00:23,  1.69pipeline/s]Generation 3 - Current best internal CV score: 0.835044881049526\nOptimization Progress:  83%|████████▎ | 100/120 [00:46<00:10,  1.86pipeline/s]Generation 4 - Current best internal CV score: 0.835044881049526\nOptimization Progress: 100%|██████████| 120/120 [00:59<00:00,  1.58pipeline/s]Generation 5 - Current best internal CV score: 0.835044881049526\nOptimization Progress: 100%|██████████| 120/120 [00:59<00:00,  1.58pipeline/s]\nBest pipeline:GradientBoostingClassifier(input_matrix, learning_rate=0.1, max_depth=10, max_features=0.5, min_samples_leaf=12, min_samples_split=5, n_estimators=100, subsample=0.7500000000000001)\n[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1.\n 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.\n 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0.\n 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0.\n 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1.\n 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0.\n 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1.\n 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.\n 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1.\n 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n 0. 1. 0. 1. 1. 0. 1. 0. 0. 0.]\n"
    }
   ],
   "source": [
    "tpot = TPOTClassifier(generations=5,population_size=20,verbosity=2)\n",
    "tpot.fit(train_features.astype(np.float64),train_labels.astype(np.float64))\n",
    "predict_y = tpot.predict(test_features)\n",
    "print(predict_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data.(pred_label)\n",
    "result = test_data.append(pd.DataFrame({'Survived':predict_y}),sort=False)\n",
    "result.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}