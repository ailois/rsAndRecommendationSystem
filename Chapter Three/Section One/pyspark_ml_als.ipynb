{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbaseconda68d4c1b18059495ab3f7b8c839aeb166",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('D:\\software\\Spark\\spark-3.0.0-preview2-bin-hadoop3.2') #这个args要指明SPARK_HOME 例如:findspark.init(\"/usr/local/spark\")\n",
    "from pyspark.sql import SparkSession\n",
    "sc=SparkSession.builder.appName(\"new_spark\").config(\"spark.executor.memory\",\"2g\").config(\"spark.executor.cores\",\"4\").config(\"spark.exector.instances\",\"1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "sql_sc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df_ratings = pd.read_csv('ratings_small.csv')\n",
    "pyspark_df_ratings = sql_sc.createDataFrame(pd_df_ratings)\n",
    "# print(pyspark_df_ratings.show(5, truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+------+-------+------+\n|userId|movieId|rating|\n+------+-------+------+\n|     1|     31|   2.5|\n|     1|   1029|   3.0|\n|     1|   1061|   3.0|\n|     1|   1129|   2.0|\n|     1|   1172|   4.0|\n|     1|   1263|   2.0|\n|     1|   1287|   2.0|\n|     1|   1293|   2.0|\n|     1|   1339|   3.5|\n|     1|   1343|   2.0|\n|     1|   1371|   2.5|\n|     1|   1405|   1.0|\n|     1|   1953|   4.0|\n|     1|   2105|   4.0|\n|     1|   2150|   3.0|\n|     1|   2193|   2.0|\n|     1|   2294|   2.0|\n|     1|   2455|   2.5|\n|     1|   2968|   1.0|\n|     1|   3671|   3.0|\n+------+-------+------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "pyspark_df_ratings = pyspark_df_ratings.drop('timestamp')\n",
    "pyspark_df_ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建ALS模型\n",
    "als = ALS(rank=3, maxIter=10, regParam=0.1, userCol='userId',itemCol='movieId', ratingCol='rating')\n",
    "model = als.fit(pyspark_df_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[Row(userId=100, recommendations=[Row(movieId=3879, rating=5.85002326965332), Row(movieId=83318, rating=5.22407341003418), Row(movieId=83411, rating=5.22407341003418), Row(movieId=67504, rating=5.22407341003418), Row(movieId=83359, rating=5.22407341003418)])]\n"
    }
   ],
   "source": [
    "# 对userId=100进行Top-N推荐\n",
    "recommendations = model.recommendForAllUsers(5)\n",
    "print(recommendations.where(recommendations.userId == 100).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ]
}